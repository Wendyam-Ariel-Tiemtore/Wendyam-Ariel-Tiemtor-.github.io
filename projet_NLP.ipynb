{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3LGBKE9gqiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques nécessaires\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "from tokenizers.decoders import BPEDecoder\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase, Sequence\n",
        "\n",
        "# Préparation d'un corpus d'exemple\n",
        "corpus = [\n",
        "    \"J'ai un chat\",\n",
        "    \"Mon chat est très mignon\",\n",
        "    \"Le chat s'est assis sur le tapis\",\n",
        "    \"Il y a aussi un chien\"\n",
        "]\n",
        "\n",
        "# Création d'un tokenizer BPE vide avec la classe `Tokenizer`\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Configuration de la normalisation du texte\n",
        "tokenizer.normalizer = Sequence([NFD(), StripAccents(), Lowercase()])\n",
        "\n",
        "# Configuration du pré-tokeniseur (séparation des mots)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Entraînement du tokenizer BPE sur le corpus d'exemple\n",
        "trainer = trainers.BpeTrainer(vocab_size=1000, show_progress=True, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"])\n",
        "\n",
        "# Entraînement du tokenizer avec le corpus\n",
        "tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "# Décodeur pour transformer les indices des tokens en texte\n",
        "tokenizer.decoder = BPEDecoder()\n",
        "\n",
        "# Test du tokenizer\n",
        "text = \"Le chat s'est assis sur le tapis\"\n",
        "encoded = tokenizer.encode(text)\n",
        "print(\"Encoded:\", encoded.tokens)\n",
        "\n",
        "# Décodeur des tokens pour retrouver le texte original\n",
        "decoded = tokenizer.decode(encoded.ids)\n",
        "print(\"Decoded:\", decoded)\n",
        "\n",
        "# Sauvegarde du tokenizer pour réutilisation\n",
        "tokenizer.save(\"bpe_tokenizer.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTrc6IfKgrWP",
        "outputId": "92ca877b-581a-4e06-dfb3-36707338c375"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: ['le', 'chat', 's', \"'\", 'est', 'assis', 'sur', 'le', 'tapis']\n",
            "Decoded: lechats'estassissurletapis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Le renard brun rapide saute par-dessus le chien paresseux.\"\n",
        "tokens_simple = text.split(' ')\n",
        "print(\"Simple Split:\", tokens_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JAqB6RHhF5_",
        "outputId": "ba50405e-845c-4980-a406-4638595c03c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Split: ['Le', 'renard', 'brun', 'rapide', 'saute', 'par-dessus', 'le', 'chien', 'paresseux.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Téléchargement des données 'point_loss'\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Utilisation du word_tokenize\n",
        "tokens_nltk = nltk.word_tokenize(text)\n",
        "print(\"NLTK Tokenisation:\", tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMyvZQsPibSH",
        "outputId": "02f74d68-3e49-4a81-8083-b064eca4c986"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenisation: ['Le', 'renard', 'brun', 'rapide', 'saute', 'par-dessus', 'le', 'chien', 'paresseux', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Chargement du modèle linguistique de base\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "print(\"SpaCy Tokenisation:\", tokens_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--D14fd8izZk",
        "outputId": "5234a7ba-11a6-46df-b30e-d0c32a7a54f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy Tokenisation: ['Le', 'renard', 'brun', 'rapide', 'saute', 'par', '-', 'dessus', 'le', 'chien', 'paresseux', '.']\n"
          ]
        }
      ]
    }
  ]
}